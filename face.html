<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Happy Emotion Detection</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            display: flex;
            flex-direction: column;
            align-items: center;
            background-color: #f4f4f4;
            margin: 0;
            padding: 20px;
        }

        h1 {
            color: #333;
        }

        video {
            border: 2px solid #333;
            border-radius: 8px;
            margin-bottom: 20px;
        }

        #result {
            font-size: 24px;
            color: #333;
        }
    </style>
</head>
<body>
    <h1>Happy Emotion Detection</h1>
    <video id="video" width="640" height="480" autoplay></video>
    <div id="result">Emotion: <span id="emotion">None</span></div>

    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
    <script>
        // Load models automatically from a URL
        async function loadModels() {
            const modelPath = 'https://your-domain.com/models/'; // Make sure to replace this with your model hosting URL
            await faceapi.nets.tinyFaceDetector.loadFromUri(modelPath);
            await faceapi.nets.faceLandmark68Net.loadFromUri(modelPath);
            await faceapi.nets.faceExpressionNet.loadFromUri(modelPath);
        }

        async function startVideo() {
            const video = document.getElementById('video');
            const stream = await navigator.mediaDevices.getUserMedia({ video: {} });
            video.srcObject = stream;

            // Start detecting faces
            video.addEventListener('play', async () => {
                const canvas = faceapi.createCanvasFromMedia(video);
                document.body.append(canvas);
                const displaySize = { width: video.width, height: video.height };
                faceapi.matchDimensions(canvas, displaySize);

                setInterval(async () => {
                    const detections = await faceapi.detectAllFaces(video, new faceapi.TinyFaceDetectorOptions()).withFaceExpressions();
                    const resizedDetections = faceapi.resizeResults(detections, displaySize);
                    canvas.width = displaySize.width;
                    canvas.height = displaySize.height;
                    faceapi.draw.drawDetections(canvas, resizedDetections);
                    faceapi.draw.drawFaceExpressions(canvas, resizedDetections);
                    
                    // Check for the presence of a happy expression
                    if (detections.length > 0) {
                        const expressions = detections[0].expressions;
                        const happyProbability = expressions.happy;

                        document.getElementById('emotion').textContent = happyProbability > 0.5 ? 'Happy' : 'Not Happy';
                    } else {
                        document.getElementById('emotion').textContent = 'None';
                    }
                }, 100);
            });
        }

        (async function() {
            await loadModels();
            await startVideo();
        })();
    </script>
</body>
</html>